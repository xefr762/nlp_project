import streamlit as st
import pandas as pd
import torch
import time
import joblib
from torch import nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from models.model_1.ml_pipeline import pipeline, decode
from sklearn.linear_model import LogisticRegression

df = pd.DataFrame({'ML(LogReg)' : 0.79, 'LSTM' : 0.77, 'Bert' : 0.91}, index=['f1_score'])

def run():
    st.title("üé¨ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Ñ–∏–ª—å–º—ã")
    st.divider()
    st.write('–ù–∞ –¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –≤–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –Ω–∞ —Ñ–∏–ª—å–º –∏ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ 3 –º–æ–¥–µ–ª—è–º:')
    st.write('1. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π ML-–∞–ª–≥–æ—Ä–∏—Ç–º LogReg (–Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF)')
    st.write('2. LSTM —Å attention-–º–µ—Ö–∞–Ω–∏–∑–º–æ–º')
    st.write('3. BERT-based –º–æ–¥–µ–ª—å (rubert-tiny-sentiment-balanced)')
    st.write('–ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –≤—Ä–µ–º–µ–Ω–µ–º –µ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ f1-macro –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π')
    st.divider()
    st.subheader('F1-Score –ø–æ –º–æ–¥–µ–ª—è–º')
    st.table(df)
    st.divider()
    model_choose = st.radio('–í—ã–±–µ—Ä–µ—Ç–µ –º–æ–¥–µ–ª—å', options = ['ML', 'LSTM', 'Bert'])
    st.write("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞, –∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç –µ–≥–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.")

    user_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:")
    
    if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"):
        if model_choose == 'ML':
            @st.cache_resource
            def load_model_ml():
                model = joblib.load('models/model_1/best_model.pkl')
                return model
            def clsf_ml(text):
                start_time = time.time()
                pred = model.predict(text)
                end_time = time.time()
                elapsed_time = end_time - start_time
                return decode(pred), elapsed_time
            if user_input:
                model = load_model_ml()
                text = pipeline(user_input)
                result, time_ml  = clsf_ml(text)
                st.write(f"–û—Ç–∑—ã–≤ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ : **{result}**. –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ : {time_ml:.4f}")
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ –æ—Ü–µ–Ω–∫–æ–π.")

        elif model_choose == 'LSTM':
            pass #TODO
        else:
            @st.cache_resource
            def load_model():
                model = ReviewBert()
                model.load_state_dict(torch.load("models/model_1/bert/clf_bert_v1.pth", map_location="cpu"))
                model.eval()
                return model
            @st.cache_resource
            def load_tokenizer():
                return AutoTokenizer.from_pretrained("cointegrated/rubert-tiny-sentiment-balanced")
            
            class ReviewBert(torch.nn.Module):
                def __init__(self):
                    super().__init__()
                    self.bert = AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-tiny-sentiment-balanced')
                    for param in self.bert.parameters():
                        param.requires_grad = False
                    self.bert.classifier = torch.nn.Sequential(
                        torch.nn.Linear(312, 256),
                        torch.nn.Tanh(),
                        torch.nn.Dropout(),
                        torch.nn.Linear(256, 128),
                        torch.nn.Tanh(),
                        torch.nn.Dropout(),
                        torch.nn.Linear(128,3)
                    )
                    for param in self.bert.classifier.parameters():
                        param.requires_grad = True
                def forward(self, input_ids, attention_mask):
                    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
                    return outputs.logits

            model = load_model()
            tokenizer = load_tokenizer()
            def tokenize_texts(texts):
                return tokenizer(texts, padding="max_length", max_length=50, truncation=True, return_tensors="pt")

            def eval_clf(text):
                start_time = time.time()
                encoded = tokenize_texts(text)
                logits = model(encoded['input_ids'], encoded['attention_mask'])
                prob = torch.softmax(logits, dim=1)
                cls = prob.argmax().item()
                end_time = time.time()
                elapsed_time = end_time - start_time
                return cls, elapsed_time

            if user_input:
                res, time_s = eval_clf(user_input)
                sentiment_dict = {"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π": 0, "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π": 1, "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π": 2}
                reversed_dict = {v: k for k, v in sentiment_dict.items()}
                st.write(f"–û—Ç–∑—ã–≤ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ : **{reversed_dict[res]}**. –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ : {time_s:.4f}")
                st.image('images/loss_ROCAUC_cls.png', caption='ROC-AUC')
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ –æ—Ü–µ–Ω–∫–æ–π.")
